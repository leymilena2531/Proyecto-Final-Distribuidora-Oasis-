# ðŸ“Š ANÃLISIS DE NEGOCIO Y GESTIÃ“N DE INVENTARIO - Ãrea IngenierÃ­a de Datos. 

## ðŸŽ¯ Objetivo Principal: Automatizar la ingesta de datos nuevos. 

Se realizÃ³ la automatizaciÃ³n de los registros nuevos cada 24 horas. Este proceso permitirÃ¡ a Distribuidora Oasis mantener sus datos de inventario y ventas siempre actualizados, optimizando la gestiÃ³n de su cadena de suministro y mejorando la toma de decisiones estratÃ©gicas. Estos datos se ven representados en Power BI, proporcionando visualizaciones y anÃ¡lisis en tiempo real.

â˜‘ï¸ Beneficios:

1. **Eficiencia mejorada:** Elimina la necesidad de procesos manuales repetitivos, reduciendo el tiempo y esfuerzo necesarios para la actualizaciÃ³n de datos.

2. **Disponibilidad de datos en tiempo real:** Proporciona acceso a datos actualizados en tiempo real, esencial para la toma de decisiones rÃ¡pidas y eficaces. Esto es crucial para gestionar el inventario de manera Ã³ptima, evitando tanto el exceso como la falta de stock.
   
4. **Escalabilidad:** Facilita la gestiÃ³n de grandes volÃºmenes de datos y permite aumentar la capacidad segÃºn las necesidades del negocio sin comprometer el rendimiento. Esto asegura que la infraestructura tecnolÃ³gica pueda crecer al mismo ritmo que la empresa.


## GOOGLE CLOUD PLATFORM (GCP) 

Es una plataforma integral de computaciÃ³n en la nube ofrecida por Google que proporciona una amplia gama de servicios de infraestructura, almacenamiento, bases de datos, anÃ¡lisis de datos, inteligencia artificial y aprendizaje automÃ¡tico para empresas de todos los tamaÃ±os y sectores.

## InformaciÃ³n del proyecto en GCP: 

**Nombre del Proyecto** `Soluciones AnalÃ­ticas PF`

**ID de proyecto:** `neural-ripple-426817-v0`

**Conjunto de Datos:** `BD_OASIS` 

**Permisos:** `Roles` modo editor en IAM y AdministraciÃ³n, para todas las Ã¡reas involucradas en el proyecto. 

**APIÂ´s utilizadas:** puede verlas [AQUÃ](https://github.com/leymilena2531/Proyecto-Final-Distribuidora-Oasis-/blob/master/Ingenieria%20de%20Datos/API.md)

## Recursos utilizados dentro de Cloud:

**BigQuery:** utilizado como Datawarehouse. Crear esquema de tablas, insertar datos y consultas SQL. 

**Cloud Storage:** almacenar las credenciales en un Bucket.

**Pub/sub:**

**CloudÂ Functions:**

**Cloud Scheduler:**

## PIPELINE:

Es una serie de pasos que los datos atraviesan desde su origen hasta su destino final, pasando por procesos de extracciÃ³n, transformaciÃ³n y carga (ETL). 

El pipeline de Distribuidora Oasis automatiza los datos asegurando que los datos de inventario y ventas sean recopilados, procesados y cargados en el sistema de gestiÃ³n de datos de manera eficiente y confiable cada 24 horas. Esto incluye:

### ðŸ›¢ï¸ EXTRACCIÃ“N: 

Los archivos CSV almacenados en Google Drive, que salieron de la limpieza de datos por el Ãrea de ÃnÃ¡lisis de Datos(link), se importaron a [Google Cloud Platform](https://cloud.google.com/?_gl=1*6gcnrv*_up*MQ..&gclid=CjwKCAjw-O6zBhASEiwAOHeGxXc4YZx6SNH1EHwvQgGmacSJnslZSK8XEbOaI-IYDAFV-nnJz4emIxoCwYcQAvD_BwE&gclsrc=aw.ds&hl=es_419), en Datawarehouse BigQuery, utilizando cÃ³digo de Python en [Google Colab](https://colab.research.google.com/drive/1j-HrMwga8oIaSLumfFZ1qPX-bo347MU1) 

![Pipeline drawio (3) 1](https://github.com/leymilena2531/Proyecto-Final-Distribuidora-Oasis-/assets/139195222/f7592c5f-a369-41fd-afa6-7c8c0f7800c3)

### ðŸ› ï¸ TRANSFORMACIÃ“N: 

**Realizada en BigQuery.**

#### 1ï¸âƒ£ Crear la estructura del esquema de 2 tablas nuevas: MOVIMIENTO_DE_INVENTARIO e INVENTARIO_REAL. 

**Crear Esquema de Tabla de Movimiento de Inventario**

```sql
-- CREAR ESQUEMA DE TABLA DE MOVIMIENTO DE INVENTARIO
1  CREATE TABLE `neural-ripple-426817-v0.BD_OASIS.MOVIMIENTO_DE_INVENTARIO` (
2      MovimientoID INT64,
3      InventarioID STRING,
4      Detalle_compraID INT64,
5      VentaID INT64,
6      TiendaID INT64,
7      ProductoID INT64,
8      Cantidad INT64
9  );
```

#### Crear Esquema de Tabla de Inventario Real

```sql
-- CREAR ESQUEMA DE TABLA DE INVENTARIO REAL
1  CREATE TABLE `neural-ripple-426817-v0.BD_OASIS.INVENTARIO_REAL` (
2      InventarioID STRING,
3      TiendaID INT64,
4      ProductoID INT64,
5      Stock INT64,
6      Fecha_fin DATE
7  );
```

#### 2ï¸âƒ£ Ingresar datos a las tablas previamente creadas. 

**Insertar datos en Tabla de Inventario Real**

```sql
-- Calcular el inventario real sumando compras y restando ventas
1  INSERT INTO `neural-ripple-426817-v0.BD_OASIS.INVENTARIO_REAL` (InventarioID, TiendaID, ProductoID, Stock, Fecha_fin)
2  SELECT 
3      CONCAT(CAST(mi.TiendaID AS STRING), '_', COALESCE(t.Ciudad, ''), '_', CAST(mi.ProductoID AS STRING)) AS InventarioID,
4      mi.TiendaID,
5      mi.ProductoID,
6      SUM(CASE WHEN mi.VentaID IS NOT NULL THEN -mi.Cantidad ELSE mi.Cantidad END) AS Stock,
7      '2016-12-31' AS Fecha_fin
8  FROM 
9      `neural-ripple-426817-v0.BD_OASIS.MOVIMIENTO_DE_INVENTARIO` AS mi
10 INNER JOIN 
11     `neural-ripple-426817-v0.BD_OASIS.TIENDA` AS t
12 ON 
13     mi.TiendaID = t.TiendaID
14 GROUP BY 
15     mi.ProductoID, mi.TiendaID, t.Ciudad;
```

**Insertar datos en Tabla Movimiento de Inventario**

```sql
1  INSERT INTO `neural-ripple-426817-v0.BD_OASIS.MOVIMIENTO_DE_INVENTARIO` (InventarioID, Detalle_compraID, VentaID, TiendaID, ProductoID, Cantidad)
2  SELECT 
3      InventarioID,
4      NULL AS Detalle_compraID,
5      NULL AS VentaID,
6      TiendaID,
7      ProductoID,
8      Stock AS Cantidad
9  FROM `BD_OASIS.Inventario_Inicial_NO_Coincidencias`

10 INSERT INTO `neural-ripple-426817-v0.BD_OASIS.MOVIMIENTO_DE_INVENTARIO` (InventarioID, Detalle_compraID, VentaID, TiendaID, ProductoID, Cantidad)
11 SELECT 
12     NULL AS InventarioID,
13     Detalle_compraID,
14     NULL AS VentaID,
15     TiendaID,
16     ProductoID,
17     Cantidad
18 FROM `neural-ripple-426817-v0.BD_OASIS.DETALLE_COMPRA`
19 WHERE Fecha_entrega > '2016-01-01';

20 INSERT INTO `neural-ripple-426817-v0.BD_OASIS.MOVIMIENTO_DE_INVENTARIO` (InventarioID, Detalle_compraID, VentaID, TiendaID, ProductoID, Cantidad)
21 SELECT 
22     NULL AS InventarioID,
23     NULL AS Detalle_compraID,
24     VentaID,
25     TiendaID,
26     ProductoID,
27     -Cantidad AS Cantidad
28 FROM `neural-ripple-426817-v0.BD_OASIS.VENTA`
29 WHERE Fecha_venta > '2016-01-01';
```


### ðŸ”‚ AUTOMATIZACIÃ“N: 

Para iniciar con el proceso de automatizaciÃ³n se realizaron pasos previos: 
1. Se importo cada archivo CSV a formato Google Sheets, guardados dentro de Google Drive. Donde la empresa ingresa registros nuevos todos los dias. 
2. Asegurarse que el archivo `.json` con las credenciales del proyecto: `neural-ripple-426817-v0-65c1b31ad608.json`, esten guardadas dentro del Bucket en Cloud Storage.

#### Pasos para realizar la **automatizaciÃ³n de ingreso de datos nuevos en la tabla COMPRA**: 

1. ðŸ‘‰ **Google Cloud Pub/Sub** (Publish/Subscribe) es un servicio de mensajerÃ­a en tiempo real y escalable que facilita la comunicaciÃ³n entre diferentes componentes de aplicaciones distribuidas en Google Cloud Platform (GCP).
Pub/Sub puede manejar grandes volÃºmenes de mensajes y escalar automÃ¡ticamente segÃºn la demanda.

Lo primero que haremos dentro de este servicio es `CREAR TEMA`.

Un tema es un canal de comunicaciÃ³n al que los editores envÃ­an mensajes. Los temas son puntos de acceso a los que los mensajes son enviados.

Elegimos un ID del tema por cada accion a realizar.

En este caso realizamos un tema llamado `Compras_oasis`.

Luego pasamos a Cloud Functions. 

2. ðŸ‘‰ **Google Cloud Functions** es una plataforma  para ejecutar cÃ³digo en respuesta a eventos, facilitando la creaciÃ³n de aplicaciones y servicios altamente escalables y eficientes, sin necesidad de gestionar ni aprovisionar servidores.

Dentro, iremos a ` CREAR FUNCION`,  en aspectos bÃ¡sicos, pusimos entorno de 2Â° gen. El nombre de la funciÃ³n que ibamos a realizar, en nuestro caso `Activar_compras`, elegimos la regiÃ³n southamerica-east1 (SÃ£o Paulo) que es la regiÃ³n la misma zona horaria que Argentina. 

En Activador, el tipo de activador elegido es Cloud Pub/Sub y el tema  serÃ¡ `Compras_oasis`, el que creamos anteriormente.

En Entorno de ejecuciÃ³n seleccionamos Python 3.10, y en cÃ³digo fuente seleccionamos `Editor directo`.
Dentro de esto se crearan automaticamente dos archivos, `main.py` y `requirements.txt`.
Punto de partida elegimos el nombre de la funciÃ³n que llamaremos a continuacion `Â¨load_data_to_bigqueryÂ¨`.

En `main.py` colocaremos el script Python que ejecute lo que necesitamos. En nuestro caso, llamamos a las entradas con fecha de hoy, de un archivo de Google SpreadSheets guardado en Google Drive, de la tabla COMPRAS. 


```sql
import os
import gspread
from google.oauth2.service_account import Credentials
from google.cloud import bigquery, storage
import pandas as pd
import datetime
import io
import json

def load_data_to_bigquery(event, context):
    """Triggered from a message on a Cloud Pub/Sub topic.
    Args:
         event (dict): Event payload.
         context (google.cloud.functions.Context): Metadata for the event.
    """
    # Nombre del bucket y del archivo en Google Cloud Storage
    BUCKET_NAME = 'archivos_oasis'
    CREDENTIALS_FILE = 'neural-ripple-426817-v0-65c1b31ad608.json'

    # Crear un cliente de Storage
    storage_client = storage.Client()
    bucket = storage_client.bucket(BUCKET_NAME)
    blob = bucket.blob(CREDENTIALS_FILE)

    # Descargar el archivo de credenciales
    credentials_content = blob.download_as_text()
    credentials_json = json.loads(credentials_content)

    # AutenticaciÃ³n con Google Sheets y BigQuery
    credentials = Credentials.from_service_account_info(credentials_json)
    gc = gspread.authorize(credentials)
    client = bigquery.Client(credentials=credentials, project=credentials.project_id)

    # ID del Google Sheet y el nombre de la hoja
    SHEET_ID = '1XUohKTqW13kRUn-HHp5mFaUljs_vCkBM6zXGAk0yNEQ'
    SHEET_NAME = 'Hoja 1'

    # Abre la hoja de cÃ¡lculo y carga los datos en un DataFrame de pandas
    spreadsheet = gc.open_by_key(SHEET_ID)
    worksheet = spreadsheet.worksheet(SHEET_NAME)
    rows = worksheet.get_all_values()
    df = pd.DataFrame(rows[1:], columns=rows[0])

    # Convierte la columna de fecha a tipo datetime si es necesario
    df['Fecha_compra'] = pd.to_datetime(df['Fecha_compra'])  # Ajusta 'fecha' al nombre real de tu columna de fecha

    # Filtra solo los datos con fecha igual a la fecha actual
    fecha_actual = datetime.date.today()
    df_filtered = df[df['Fecha_compra'].dt.date == fecha_actual]

    if not df_filtered.empty:
        # Define el conjunto de datos y la tabla en BigQuery
        dataset_id = 'BD_OASIS'
        table_id = 'COMPRA'

        # Carga el DataFrame filtrado en BigQuery
        table_ref = client.dataset(dataset_id, project=client.project).table(table_id)
        job_config = bigquery.LoadJobConfig(
            write_disposition=bigquery.WriteDisposition.WRITE_APPEND,
            source_format=bigquery.SourceFormat.CSV,
            autodetect=True,
        )

        # Convierte el DataFrame filtrado a un archivo CSV temporal
        csv_data = df_filtered.to_csv(index=False)

        # Carga los datos desde el CSV temporal a BigQuery
        load_job = client.load_table_from_file(
            io.StringIO(csv_data), table_ref, job_config=job_config
        )

        # Espera a que el trabajo de carga termine
        load_job.result()

        print(f'Datos cargados en BigQuery para la fecha: {fecha_actual}')
    else:
        print('No se encontraron datos para cargar hoy.')

    # Registro del evento recibido
    print(f'Evento recibido: {event}')

    # Ejemplo de registro del contexto
    print(f'Contexto: {context}')

    # Ejemplo de devoluciÃ³n de mensaje para Pub/Sub
    return 'Procesamiento completado.'
```

En `requirements.txt`  se escribe todas las dependencias que el cÃ³digo necesita para ejecutarse correctamente en Google Cloud Functions. 


```sql
google-cloud-storage
google-cloud-bigquery
google-auth
google-auth-oauthlib
google-auth-httplib2
gspread==4.0.1  # Especificar la versiÃ³n si lo necesitas
pandas==1.4.0   # Especificar una versiÃ³n de Pandas compatible
numpy==1.22.3   # Especificar una versiÃ³n de NumPy compatible
```

Luego de esto, haremos probar funciÃ³n.

Una vez que nos avisa que el codigo estÃ¡ correcto, procederemos a irnos pasar a Google Cloud Scheduler.

3. ðŸ‘‰ **Google Cloud Scheduler** es un servicio de planificaciÃ³n de tareas basado en la nube que permite ejecutar trabajos cron (jobs) de manera automatizada y programada. Es ideal para tareas recurrentes y programadas que necesitan ejecutarse a intervalos especÃ­ficos o en momentos determinados.

En nuestro caso, publicarÃ¡ el Pub/Sub creado inicial. 

Primero, definimos el programa.
Le pondremos un nombre al programa, en nuestro caso serÃ¡ `Actualizar_compras`, elegimos la misma region que veniamos trabajando, le agregamos una breve descripciÃ³n de lo que harÃ¡ el programa, y programaremos la frecuencia.

La frecuencia se especifica con el formato Cron, que funciona de la siguiente forma:

```sql
* * * * *
â”‚ â”‚ â”‚ â”‚ â”‚
â”‚ â”‚ â”‚ â”‚ â””â”€ DÃ­a de la semana (0 - 7) (Domingo = 0 o 7)
â”‚ â”‚ â”‚ â””â”€â”€â”€ Mes (1 - 12)
â”‚ â”‚ â””â”€â”€â”€â”€â”€ DÃ­a del mes (1 - 31)
â”‚ â””â”€â”€â”€â”€â”€â”€â”€ Hora (0 - 23)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€ Minuto (0 - 59)
```

En nuestro caso, para "59 23 * * *" significarÃ¡ que todos los dÃ­as las 23:59 se ejecutarÃ¡ la acciÃ³n, donde todos las filas con fecha del dia entrarÃ¡n a la tabla COMPRA. 

Una vez finalizada la programaciÃ³n, se ejecutarÃ¡ la carga automatica de datos.

## ðŸ“Š CARGA: 

Los registros nuevos que se ingresen por dÃ­a serÃ¡n actualizados en cada tabla correspondiente en el Datawarehouse, el cual es conectado con Power Bi, para visualizar en tiempo real los datos de manera visual, calculando mÃ©tricas de negocio, para su mejor entendimiento y eficacia. 


#### TecnologÃ­as y Herramientas Utilizadas a lo largo del pipeline:

Google Drive, Google Spreadsheets, Google Colab, Google Cloud Platform, BigQuery, Cloud Storage, Pub/Sub, CloudÂ Functions, Cloud Scheduler. Draw.io. 

